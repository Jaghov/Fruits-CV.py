class WeightNormConv2d(nn.Module):
    def __init__(self, in_channel, out_channel, kernel_size, stride=1, padding=0, bias=True):
        super().__init__()
        self.conv = nn.utils.weight_norm(nn.Conv2d(in_channel, out_channel, kernel_size, stride=stride, padding=padding, bias=bias))

    def forward(self, x):
        return self.conv(x)



class ConvBlock(nn.Module):
  def __init__(self, in_channels, hidden_features=512):
    super().__init__()
    layers =  [
      WeightNormConv2d(in_channels, hidden_features, kernel_size=3, padding=1),
      nn.ReLU(),
      WeightNormConv2d(hidden_features, hidden_features, kernel_size=1),
      nn.ReLU(),
      WeightNormConv2d(hidden_features, 2*in_channels, kernel_size=3, padding=1),
    ]
    self.net = nn.Sequential(*layers)

    self.net[0].conv.weight.data.normal_(0, 0.05)
    self.net[0].conv.bias.data.zero_()

    self.net[-1].conv.weight.data.normal_(0, 0.05)
    self.net[-1].conv.bias.data.zero_()

  def forward(self, x):
    return self.net(x)
